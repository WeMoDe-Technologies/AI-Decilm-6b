# -*- coding: utf-8 -*-
"""DeciLM-6B-Instruct.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y8ANESVn23_2TJaGUNFQ2i-9Y__NQ8Ry
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install torch
# !pip install huggingface_hub
# !pip install transformers
# !pip install accelerate
# !pip install bitsandbytes
# !pip install scipy

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, TextIteratorStreamer
import csv

model_id = 'Deci/DeciLM-6b-instruct'

model = AutoModelForCausalLM.from_pretrained(model_id,
                                             device_map="auto",
                                             trust_remote_code=True
                                             )

tokenizer = AutoTokenizer.from_pretrained(model_id)

tokenizer.pad_token = tokenizer.eos_token

SYSTEM_PROMPT_TEMPLATE = """
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Response:
"""

# Function to construct the prompt using the new system prompt template
def get_prompt_with_template(message: str) -> str:
    return SYSTEM_PROMPT_TEMPLATE.format(instruction=message)

# Function to handle the generation of the model's response using the constructed prompt
def generate_model_response(message: str) -> str:
    prompt = get_prompt_with_template(message)
    inputs = tokenizer(prompt, return_tensors='pt')
    if torch.cuda.is_available():  # Ensure input tensors are on the GPU if model is on GPU
        inputs = inputs.to('cuda')
    output = model.generate(**inputs,
                            max_new_tokens=100, #3000
                            num_beams=5,
                            no_repeat_ngram_size=4,
                            early_stopping=True
                            )
    return tokenizer.decode(output[0], skip_special_tokens=True)

# Function to extract the content after "### Response:"
def extract_response_content(full_response: str) -> str:
    response_start_index = full_response.find("### Response:")
    if response_start_index != -1:
        return full_response[response_start_index + len("### Response:"):].strip()
    else:
        return full_response

# Main function to get the model's response and extract the content after "### Response:"
def get_response_with_template(message: str) -> str:
    full_response = generate_model_response(message)
    return extract_response_content(full_response)

fieldnames = ['ProductId','ProductName','OldImageId','OldImageSrc','ImageDescription', 'ImageCaption']
with open('product_image_details.csv', mode = 'r', newline='', encoding='utf-8') as src_file, open('product_image_caption.csv', mode='a', newline='', encoding='utf-8') as tgt_file:
    reader = csv.DictReader(src_file)
    writer = csv.DictWriter(tgt_file, fieldnames=fieldnames)
    i = 1
    for row in reader:
        desc = row['ImageDescription']
        productname = row['ProductName']
        prompt = f"Help me summarize the image description of {productname} in less that 120 characters. {desc}"
        caption = get_response_with_template(desc)
        print(prompt)
        print(i)
        print(caption)
        row['ImageCaption'] = caption
        writer.writerow(row)
        i= i+1
    